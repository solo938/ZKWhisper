{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNTT7I68KaZaOofTj2V2q/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solo938/ZKWhisper/blob/main/ZKWhisper_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit langchain langchain-community langchain-openai chromadb openai tiktoken pyngrok pysqlite3-binary -q\n"
      ],
      "metadata": {
        "id": "BBmPTROmQoZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base_content = \"\"\"\n",
        "# Zero-Knowledge Proofs, Noir, and Tornado Cash Knowledge Base\n",
        "\n",
        "## 1. Zero Knowledge Proofs (ZKPs) Fundamentals\n",
        "**Definition:** A cryptographic method allowing a \"Prover\" to convince a \"Verifier\" that a statement is true without revealing the secret information (witness) underlying the statement.\n",
        "\n",
        "**Core Properties:**\n",
        "1. **Completeness:** If the statement is true and the prover is honest, the verifier will be convinced.\n",
        "2. **Soundness:** If the statement is false, a dishonest prover cannot convince the verifier (except with negligible probability).\n",
        "3. **Zero-Knowledge:** The verifier learns nothing other than the fact that the statement is true.\n",
        "\n",
        "## 2. The Noir Programming Language\n",
        "**Overview:**\n",
        "Noir is a Domain Specific Language (DSL) for generating zero-knowledge proofs. It is backend-agnostic and compiles to an intermediate representation called **ACIR**.\n",
        "\n",
        "**Key Features:**\n",
        "* **Rust-like Syntax:** Uses `fn`, `let`, `struct`.\n",
        "* **Tooling (Nargo):**\n",
        "    * `nargo new`: Create project.\n",
        "    * `nargo check`: Compile/check constraints.\n",
        "    * `nargo prove`: Generate witness and proof.\n",
        "    * `nargo verify`: Verify the proof.\n",
        "\n",
        "## 3. Tornado Cash Architecture\n",
        "**Purpose:** An Ethereum privacy solution (mixer) that breaks the on-chain link between recipient and sender addresses.\n",
        "\n",
        "**Core Mechanism:**\n",
        "1. **Deposit:** User generates a secret (Randomness `r` and Nullifier `k`), computes hash `C = Hash(k, r)`, and adds `C` to a Merkle Tree.\n",
        "2. **Withdraw:** User generates a ZK-SNARK proof proving knowledge of `k` and `r` in the Merkle root. The **Nullifier Hash** (`Hash(k)`) prevents double-spending.\n",
        "\n",
        "## 4. Security & Auditing\n",
        "**Common Vulnerabilities:**\n",
        "* **Under-constrained Circuits:** Failing to constrain inputs sufficiently allows malicious provers to forge proofs.\n",
        "* **Input Aliasing:** Multiple inputs mapping to the same value in modular arithmetic.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"knowledge_base.md\", \"w\") as f:\n",
        "    f.write(knowledge_base_content)\n",
        "\n",
        "print(\" Knowledge base created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZy2FfM9QrdB",
        "outputId": "1aae164f-3f78-4bbd-f84f-8271b375e0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Knowledge base created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# --- CRITICAL FIX FOR COLAB (Must be at top) ---\n",
        "import pysqlite3\n",
        "import sys\n",
        "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
        "# -----------------------------------------------\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LCEL imports\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Page Configuration\n",
        "st.set_page_config(page_title=\"ZK RAG Bot\", layout=\"wide\")\n",
        "st.title(\" ZK Contextual RAG Bot\")\n",
        "\n",
        "# Sidebar - API Key Input\n",
        "with st.sidebar:\n",
        "    st.header(\"Configuration\")\n",
        "    api_key = st.text_input(\"Enter OpenAI API Key\", type=\"password\")\n",
        "\n",
        "    if not api_key:\n",
        "        st.warning(\"‚ö†Ô∏è Please enter your OpenAI API Key to continue.\")\n",
        "        st.stop()\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    st.success(\" API Key loaded\")\n",
        "\n",
        "# Initialize RAG Chain (cached for efficiency)\n",
        "@st.cache_resource\n",
        "def setup_rag():\n",
        "    \"\"\"\n",
        "    Initialize the RAG chain with:\n",
        "    1. Document loading\n",
        "    2. Text splitting\n",
        "    3. Vector embeddings\n",
        "    4. ChromaDB storage\n",
        "    5. LLM + LCEL chain setup\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with st.spinner(\"üîÑ Loading documents...\"):\n",
        "            loader = TextLoader(\"knowledge_base.md\")\n",
        "            docs = loader.load()\n",
        "            st.sidebar.info(f\"üìÑ Loaded {len(docs)} document(s)\")\n",
        "\n",
        "        with st.spinner(\"‚úÇÔ∏è Splitting documents...\"):\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "            splits = splitter.split_documents(docs)\n",
        "            st.sidebar.info(f\" Created {len(splits)} chunks\")\n",
        "\n",
        "        with st.spinner(\"Creating embeddings...\"):\n",
        "            embeddings = OpenAIEmbeddings()\n",
        "            vectorstore = Chroma.from_documents(\n",
        "                splits,\n",
        "                embeddings,\n",
        "                persist_directory=\"./chroma_db\"\n",
        "            )\n",
        "            vectorstore.persist()\n",
        "\n",
        "        with st.spinner(\" Setting up LLM and Chains...\"):\n",
        "            llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "            qa_system_prompt = \"\"\"You are a helpful assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Keep the answer concise and relevant.\n",
        "\n",
        "Context: {context}\"\"\"\n",
        "\n",
        "            qa_prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", qa_system_prompt),\n",
        "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "                (\"human\", \"{input}\"),\n",
        "            ])\n",
        "\n",
        "            # Create a simple chain using LCEL\n",
        "            from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "            def format_docs(docs):\n",
        "                return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "            rag_chain = (\n",
        "                RunnablePassthrough.assign(\n",
        "                    context=lambda x: format_docs(retriever.get_relevant_documents(x[\"input\"]))\n",
        "                )\n",
        "                | qa_prompt\n",
        "                | llm\n",
        "            )\n",
        "\n",
        "        st.sidebar.success(\" RAG setup complete!\")\n",
        "        return {\"chain\": rag_chain, \"retriever\": retriever}\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\" Setup Error: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "# Initialize session state\n",
        "if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = setup_rag()\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat history\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.write(msg[\"content\"])\n",
        "\n",
        "# Chat input and response\n",
        "if prompt := st.chat_input(\"Ask about Noir, Tornado Cash, or Zero-Knowledge Proofs...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "    formatted_chat_history = []\n",
        "    for msg in st.session_state.messages[:-1]:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            formatted_chat_history.append(HumanMessage(content=msg[\"content\"]))\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            formatted_chat_history.append(AIMessage(content=msg[\"content\"]))\n",
        "\n",
        "    if st.session_state.conversation:\n",
        "        with st.spinner(\" Thinking...\"):\n",
        "            try:\n",
        "                chain = st.session_state.conversation[\"chain\"]\n",
        "                result = chain.invoke({\n",
        "                    \"input\": prompt,\n",
        "                    \"chat_history\": formatted_chat_history\n",
        "                })\n",
        "                answer = result.content if hasattr(result, 'content') else str(result)\n",
        "\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "                with st.chat_message(\"assistant\"):\n",
        "                    st.write(answer)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\" Error generating response: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic1EbIpEQwUr",
        "outputId": "104e83b6-785a-4eb8-f351-e35660118f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"NGROK AUTH TOKEN\")\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\" View your RAG Bot here: {public_url}\")\n",
        "print(f\" Logs will appear below...\\n\")\n",
        "\n",
        "!streamlit run app.py &> logs.txt &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeTw50SWQ2dt",
        "outputId": "5e88e1ba-5cc4-4547-93ea-aca1bb4f7de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ View your RAG Bot here: NgrokTunnel: \"https://unmulcted-oneirocritical-lawerence.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "üìù Logs will appear below...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}